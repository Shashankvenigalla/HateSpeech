{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be08a42-e98d-4bb1-bfa3-dc3474af8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7cbcfe-34e0-4c75-92ca-9c69cf58bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ded887-0369-41e7-aaf7-faa754e42056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4203fdb5-4a5b-4abb-b2af-81e9b59d57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='label', data=df_twitter, palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01041833-8759-471f-a545-777df88947ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc88b69f-94a3-4903-a33b-236c280bd14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1813fb16-27ef-4f59-87ed-a8f32e5bf59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea17f581-4017-4ec9-ad3c-2780688e761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive=pd.read_csv(\"labeled_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c46962-c3ff-4683-88bc-689b056d5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08b2f5b-43b0-4f6d-bf97-4b36a3fba90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1364521-0b19-4cee-a13a-841c2e78fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8b44d-e06a-46f8-9506-f08b869994d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.drop(['Unnamed: 0','count','hate_speech','offensive_language','neither'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69046147-bb29-44b5-a8f2-1f49ec3946c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72bc9c6-5a83-4bc3-be5d-0f127e0cc7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e833e4-9f93-4766-ae1e-5a6a87ce04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='class', data=df_offensive, palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d8484-efa4-4d11-bf1b-2e702b9d38a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive[df_offensive['class']==0]['class']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d23dff-19bd-457f-9d00-d4032b467662",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477b172-dd3a-45f3-ac34-60e4a90b2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38ffca-e446-46ad-a9df-a736ae095d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive[df_offensive['class']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fcc56b-2099-44d7-83da-c4f5a7442d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive[\"class\"].replace({0: 1}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4964f2f-7210-4b3a-b9e1-619d658534a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac05fe-a276-4ec3-be8c-5a001fb26dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='class', data=df_offensive, palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a73ea96-34bf-478a-8162-d78794845987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive[df_offensive['class']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f79ca-0cf7-4544-8283-df6888062e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive[\"class\"].replace({2: 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b2919-be3f-4578-a435-096995ab0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x='class',data=df_offensive, palette='Set2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cc800-7f4b-4bcd-8d24-2ba487c2120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.rename(columns ={'class':'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c575603-34de-4fb7-a609-48be2a90875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f7244-24d5-4d4c-95cb-847e4d836133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.iloc[0]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8053a9-5329-4f95-99a6-9ebfc69d4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offensive.iloc[5]['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf3bb3-3fab-43fa-8395-d5696f799bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame=[df_twitter,df_offensive]\n",
    "df = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6caad41-07da-41f4-82cc-51d71ac249a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82403ee7-994c-48be-b378-491c4e0df6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='label',data=df, palette='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b9b7c-2a96-44ce-82ba-89ed236c53dc",
   "metadata": {},
   "source": [
    "Confusion Matrix of Models Used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c26e10-be95-4237-b36f-004ebcf33c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, res2)\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Hate', 'Hate/Abusive'],\n",
    "            yticklabels=['No Hate', 'Hate/Abusive'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311fe149-8e0e-431e-bef7-f3e79da0088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  plt.figure(figsize=(8, 6))\n",
    "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "              xticklabels=['No Hate', 'Hate/Abusive'],\n",
    "              yticklabels=['No Hate', 'Hate/Abusive'])\n",
    "  plt.xlabel('Predicted Label')\n",
    "  plt.ylabel('True Label')\n",
    "  plt.title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5875f-886f-45fb-823e-5c0cbc17df29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for MultinomialNB with CountVectorizer\n",
    "plot_confusion_matrix(y_test, prediction_vectorizer, 'MultinomialNB with CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c55185-c731-433c-ac6d-ee685db20761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for MultinomialNB with TF-IDF\n",
    "plot_confusion_matrix(y_test, prediction_tfidf, 'MultinomialNB with TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5b203-3add-41ab-8a1f-c8474da9816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for XGBoost with CountVectorizer\n",
    "plot_confusion_matrix(y_test, xgb_predictions_vectorizer, 'XGBoost with CountVectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca73f1b-d591-462a-97f7-770caf5a58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for XGBoost with TF-IDF\n",
    "plot_confusion_matrix(y_test, xgb_predictions, 'XGBoost with TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090acb4-2c29-4fb4-9ba3-688426c6d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for LSTM Model (First LSTM Model)\n",
    "plot_confusion_matrix(y_test, res, 'LSTM Model (First LSTM Model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe05b38-9d89-469e-bff3-7463778c5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for LSTM Model (Second LSTM Model)\n",
    "plot_confusion_matrix(Y_test, res2, 'LSTM Model (Second LSTM Model)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3f84ed-d944-4591-bb7a-4a2b4058164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3487f96-087b-430c-8695-50324642cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868ed3b-9979-42f1-808a-84afb080abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopword=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b0e877-bff5-4300-8d0c-0103ae217d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9625b05e-db66-4ace-875f-0da969d6f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet']=df['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4defe-6c26-4459-adb1-13481b04ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1da2e-8ffb-4726-a36b-75c8cca40508",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbd77e-e2f9-465d-be9c-f13f419ae3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27437029-434b-403d-b209-3bffd96e30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordcloud(df):\n",
    "    comment_words=\"\"\n",
    "    for val in df.tweet:\n",
    "        val = str(val).lower()\n",
    "\n",
    "\n",
    "        comment_words += \" \".join(val)+\" \"\n",
    "    print(comment_words[0:100])\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,min_font_size = 10).generate(comment_words)\n",
    "\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae8795-20fe-499e-a1b6-b54eed08341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['tweet']\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfc5e2-7345-4e46-902d-e9b81559784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
    "print(len(x_train), len(y_train))\n",
    "print(len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981a620b-4465-43fd-87c4-6ca8524017da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english', ngram_range=(1,5))\n",
    "x_train_vectorizer=count.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb06f33e-c86b-4bdf-bd2e-e75e41a47b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_vectorizer=count.transform(x_test)\n",
    "x_train_vectorizer.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ff395-00f3-4fc7-a35f-f1c4b60d68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34605b5e-1549-4a20-8b30-a6f3e1024c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "x_train_tfidf = tfidf.fit_transform(x_train_vectorizer)\n",
    "x_train_tfidf.toarray()\n",
    "x_test_tfidf = tfidf.transform(x_test_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3efb503-d344-43e0-aa63-4de232182922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc413e2d-7c77-40ef-aced-2ea96cc24c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vectorizer= MultinomialNB().fit(x_train_vectorizer, y_train)\n",
    "prediction_vectorizer=model_vectorizer.predict(x_test_vectorizer)\n",
    "print(confusion_matrix(y_test,prediction_vectorizer))\n",
    "print (classification_report(y_test, prediction_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1acf8d-a4a1-4add-8428-6b35f5c6008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tfidf= MultinomialNB().fit(x_train_tfidf, y_train)\n",
    "prediction_tfidf=model_tfidf.predict(x_test_tfidf)\n",
    "print (classification_report(y_test, prediction_tfidf))\n",
    "print(confusion_matrix(y_test,prediction_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ff758-8537-4a15-b95b-bfb23d5b0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3e083-a57a-460c-9e86-38ed650d8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_model=xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        max_depth=7,\n",
    "        n_estimators=80,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915143ce-774e-4f99-b84e-7e8dc198aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_vectorizer = xgb_model.fit(x_train_vectorizer, y_train)\n",
    "xgb_predictions_vectorizer=xgb_model_vectorizer.predict(x_test_vectorizer)\n",
    "print(confusion_matrix(y_test,xgb_predictions_vectorizer))\n",
    "print (classification_report(y_test, xgb_predictions_vectorizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355c2f7-f042-4514-80da-a40dbe39b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427ba679-dc25-4970-a914-d87d19206127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e62fca9-31bb-42b0-998b-fe22af132297",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000\n",
    "max_len = 300\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248552ed-7e7b-4421-b32a-97ccd9cef016",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Set a vocabulary size (based on the embedding layer's limit)\n",
    "max_words = 5000  # Maximum number of words to keep, based on word frequency.\n",
    "max_len = 100  # Maximum sequence length\n",
    "\n",
    "# Tokenizer with a limit on the number of words (vocabulary)\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "# Convert text sequences to matrix\n",
    "sequences_matrix = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences_matrix, maxlen=max_len)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100, input_length=max_len))  # Ensure max_words matches your tokenizer limit\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    model.fit(sequences_matrix, y_train, batch_size=128, epochs=40,\n",
    "              validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0001)])\n",
    "\n",
    "    # Prepare test data\n",
    "    test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "    test_sequences_matrix = sequence.pad_sequences(test_sequences, maxlen=max_len)\n",
    "\n",
    "    # Evaluate model\n",
    "    accr = model.evaluate(test_sequences_matrix, y_test)\n",
    "    print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0], accr[1]))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7baacb-8c9c-4585-a34b-39c6fdfb69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Activation, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define parameters\n",
    "max_words = 50000  # Adjust this based on your vocabulary size\n",
    "max_len = 300  # Input length of each sample\n",
    "\n",
    "def RNN():\n",
    "    inputs = Input(name='inputs', shape=[max_len])\n",
    "    layer = Embedding(max_words, 50, input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256, name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1, name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs, outputs=layer)\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = RNN()\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='model.weights.h5',  # Specify a valid filepath ending with .weights.h5\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e02225-13f7-4fe4-87b7-4ae438c05ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "stop = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=5\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='model.weights.h5',  # Specify a valid filepath ending with .weights.h5\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ecdac-0740-46d6-abde-5e1220ca5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define parameters\n",
    "max_words = 5000  # Vocabulary size\n",
    "max_len = 100  # Sequence length\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "sequences_matrix = tokenizer.texts_to_sequences(x_train)\n",
    "sequences_matrix = pad_sequences(sequences_matrix, maxlen=max_len)  # Make sure this import exists!\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_len))  # Ensure max_words is valid\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "    history = model.fit(sequences_matrix, y_train, batch_size=1024, epochs=10,\n",
    "                        validation_split=0.2)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683fa61-30ef-482c-a728-133eb6251812",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0281a0ec-fa4e-4d56-aa22-f139cf78c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15eb698-07d6-48aa-97ab-6c38434783f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_prediction=model.predict(test_sequences_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01db626-88bf-4105-9b19-6e1b4c6486b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=[]\n",
    "for prediction in lstm_prediction:\n",
    "    if prediction[0]<0.5:\n",
    "        res.append(0)\n",
    "    else:\n",
    "        res.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e930f7-f6b3-401c-a380-caec66dafd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c4f3e-152c-4ed0-b81d-b8a426690450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2c5b1-995a-41b2-9a51-5ad2bf6b157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"hate&abusive_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f51a2d-b60a-4011-a047-2736dde94f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# ✅ Load model and assign it to a variable\n",
    "model = load_model(\"./hate&abusive_model.h5\")  # ✅ Fix: store the model instance\n",
    "\n",
    "# ✅ Load tokenizer\n",
    "tokenizer_path = \"tokenizer.pickle\"\n",
    "with open(tokenizer_path, 'rb') as handle:\n",
    "    load_tokenizer = pickle.load(handle)\n",
    "\n",
    "# ✅ Define stopwords and stemmer\n",
    "stopword = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# ✅ Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = \" \".join([word for word in text.split() if word not in stopword])\n",
    "    \n",
    "    # Apply stemming\n",
    "    text = \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# ✅ Sample Test\n",
    "test = [\"I hate my country\"]\n",
    "test_cleaned = [clean_text(text) for text in test]\n",
    "print(\"Cleaned Text:\", test_cleaned)\n",
    "\n",
    "# ✅ Convert text to sequence\n",
    "seq = load_tokenizer.texts_to_sequences(test_cleaned)\n",
    "padded = pad_sequences(seq, maxlen=100)  # ✅ Ensure maxlen matches model input_length\n",
    "print(\"Tokenized Sequence:\", seq)\n",
    "\n",
    "# ✅ Use the correctly loaded model to predict\n",
    "pred = model.predict(padded)  # ✅ Fix: Use `model`, NOT `load_model`\n",
    "print(\"Prediction:\", pred)\n",
    "\n",
    "# ✅ Interpret Results\n",
    "if pred < 0.5:\n",
    "    print(\"No Hate\")\n",
    "else:\n",
    "    print(\"Hate and Abusive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1fb8f2-2bd3-4fde-b85a-9ed69886556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame=[df_twitter,df_offensive]\n",
    "df2 = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4d9c4-98b8-4051-9012-f7522d54fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d67725-de65-499b-bf33-2bb9ac26ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "df2['tweet'] = df2['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503d5b3-7caf-479f-b98d-802ce8864fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be6695-33eb-4c2e-b643-8f3095742a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer2 = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer2.fit_on_texts(df2['tweet'].values)\n",
    "word_index = tokenizer2.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f17a3-0990-435f-9eaf-90ed2be3efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer2.texts_to_sequences(df2['tweet'].values)\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a478ff5-6c24-465e-90d2-ec9a4e3861a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484d56e-6fa5-4712-849c-b16eec0cf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c3436-9261-4944-bab1-cc07afb4c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Define model\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model2.add(SpatialDropout1D(0.2))\n",
    "model2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# ✅ Fix: Properly import and use RMSprop\n",
    "model2.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# ✅ Fix: Define Callbacks Properly\n",
    "stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 10\n",
    "batch_size = 1024\n",
    "\n",
    "# Train the model\n",
    "history2 = model2.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                       validation_split=0.2, callbacks=[stop, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8277f-f424-49ac-bcd0-40f288dc0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sequences2 = tokenizer2.texts_to_sequences(X_test)\n",
    "# test_sequences_matrix2 = sequence.pad_sequences(test_sequences2,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "accr = model2.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848e9bc-a6c2-4179-8d04-9986dd200070",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_prediction2=model2.predict(X_test)\n",
    "print(lstm_prediction2[0:5])\n",
    "res2=[]\n",
    "for prediction in lstm_prediction2:\n",
    "    if prediction[0]<0.45:\n",
    "        res2.append(0)\n",
    "    else:\n",
    "        res2.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8868c4-e907-49a3-8952-e40c208ba38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791047d-392d-42ff-a28b-c30934b919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test,res2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae84e1-ee02-424d-a419-76c57c856a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'I love this country'\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "test=[clean_text(test)]\n",
    "print(test)\n",
    "seq = tokenizer2.texts_to_sequences(test)\n",
    "padded = sequence.pad_sequences(seq, maxlen=250)\n",
    "print(seq)\n",
    "pred = model2.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "if pred<0.45:\n",
    "    print(\"no hate\")\n",
    "else:\n",
    "    print(\"hate and abusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522ae05-0fd2-48b7-96da-f3294ae0e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'I hate this country'\n",
    "def clean_text(text):\n",
    "\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = text.replace('x', '')\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "test=[clean_text(test)]\n",
    "print(test)\n",
    "seq = tokenizer2.texts_to_sequences(test)\n",
    "padded = sequence.pad_sequences(seq, maxlen=250)\n",
    "print(seq)\n",
    "pred = model2.predict(padded)\n",
    "print(\"pred\", pred)\n",
    "if pred<0.45:\n",
    "    print(\"no hate\")\n",
    "else:\n",
    "    print(\"hate and abusive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098996c-f43c-4726-b34f-7b71f422e477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7af9e4-6fa2-4ece-af6c-c1e03d49f46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27eb7b39-da2a-4a14-9b6c-09e53c91eb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52569e35-379d-4f1b-bd33-aa00e6295d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7020411-edbe-4d1c-8839-b6a2da2d6b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633cbdd-42ff-4de1-8738-79d7a753e654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c7b77-0f8a-49bb-9a51-d4ec1e8d11c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07cc9d-7260-4218-ac3f-dc8ae255a635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940fca0-d0a5-48e4-a07e-408f3e2c1446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c41f11-d9d2-4c65-adfc-7b5f3030c8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a781ae6-3d8c-460a-8238-30b28dc40332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23219583-d86e-4183-a2f9-55ac2fb337a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3c2cf-4084-4de6-8f03-aeb186d3a061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9728cc-a607-4723-8daa-371da1e1d616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd1d04-1316-43ca-871e-a9155172e455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0822ee65-2de6-4117-b301-65cb6955f330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31793303-415a-4e13-9dfe-91c9d8ec46d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796b0bc-300c-4a6b-af74-4817efc56a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d56f2-279a-4d52-9082-174df03cbd01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ead399-3e02-447b-8eee-b851c07bc77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713f872-44d7-40e5-b3db-5e61dfe251ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245dc30-b2e4-4a92-adb9-23f1c91abc05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
